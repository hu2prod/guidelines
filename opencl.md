# opencl performance guide
Не знаешь какой вариант выбрать, выбирай **жирный**

## Перед тем как читать советы
У каждого kernel'а есть свое узкое звено в системе
 * memory bound read bandwidth. kernel не успевает читать
 * memory bound write bandwidth. kernel не успевает писать
 * memory bound read tx. kernel не может сделать достаточного количества запросов на чтение
 * (экзотика) memory bound write tx
 * memory bound bank conflict. Доступы к памяти не уравновешены по банкам памяти/чипам памяти
 * compute bound pure. Просто не хватает вычислительной дури
 * compute bound branching. Обычно злоупотребление branching'ом
 * loc mem bound. Когда scheduler не может выделить достаточное количество вычислителей, потому что на каждого не хватит регистров.

Каждый kernel имеет следующие метрики которые можно выбирать в качестве главных
 * data per second (throughput)
   * thread per second (сколько thread'ов можно запустить и закончить)
 * kernel per second (если kernel запускается много раз)
 * kernel latency при фиксированном объеме данных

Бывают следующие варианты с узким звеном
 * Когда можно поменять характеристики kernel'а, что узким звеном станет другой компонент и станет лучше по выбранной метрике
 * Когда нельзя

Какой типичный сценарий для оптимизации
 * Метрика kernel latency, а вызывается kernel 1-N раз на приходящую порцию данных (например на кадр от видеокамеры)
 * Объем данных > 10e6 единиц
   * Если меньше стоит подумать а имеет ли смысл дискретная видеокарта, может лучше APU или C++ intristics.
 * Весь процесс является pipeline'ом и можно построить DAG вычислений
 * Основная цель сделать latency от первой полученной порции данных до выданого результата как можно меньше
   * Идеально меньше времени 1 кадра. Для 60 FPS это 16 ms
 * Если не получается уложиться в 1 кадр, то обработка второго кадра не должна мешать завершению обработки первого кадра
   * latency не должно расти
 * Операции внутри одного kernel'а почти не зависимы друг от друга
   * Возможно имеют какой-то общий вход

Что делать в общем случае
 * Все версии kernel'ов от самой базовой до самой оптимизированной желательно сохранять
 * Измерения имеет смысл проводить когда время выполнения 1 kernel'а на типичных данных больше 1 ms
   * Если меньше, то не надо искусственно увеличивать объем данных, иначе рискуем получить субоптимальную систему
 * После каждой оптимизации необходимо проверять kernel'ы на то, что
   * производительность стала лучше
   * корректность всё еще в силе
 * Какие тесты желательно иметь
   * корректность всех/большинства оптимизаций kernel'ов
   * benchmark разных оптимизаций kernel'ов
   * stress test, который может нагрузить систему на определённое количество порций данных или на определённое время.
     * С вариантом бесконечного демо
 * Если в результате всех оптимизаций получилось ускорение относительно cpu меньше чем x100, стоит задуматься, что что-то не так
   * Плохая видеокарта или проблемы с hardware
     * глючное
     * старое
   * Плохая прокладка между клавиатурой и монитором
     * Не читал документацию
   * Неудачный алгоритм или реализация
     * Больше вычислений чем надо
     * Недостаточно параллельные вычисления
   * Плохая задача
     * Обычно только для научных задач
   * Возможно, стоит переместить обратно на CPU и оптимизировать там используя векторизацию, intristics и многопоточность
     * Убираяя пересылку данных на видеокарту и обратно получим сравнимые характеристики и независимость от доп. hardware
   * Возможно, стоит посмотреть в сторону
     * Xeon PHI
     * FPGA
     * APU

## Как бороться с bottleneck'ами
 * memory bound read/write bandwidth
   * Меньше читать/писать
   * Читать/писать в адреса памяти, которые рядом
   * Читать/писать в адреса памяти, которые не дают bank conflict
 * memory bound tx
   * Завести 2 буфера. Один на GPU, второй на CPU и чередовать чтения.
 * memory bound bank conflict
   * См. документацию
   * См. альтернативные реализации
 * Сompute bound pure
   * Использование специальных инструкций (см. документацию по микрокодам AMD) 
   * Посмотреть что происходит в CodeXL
   * Купить видеокарту по-лучше
   * Поставить 2 видеокарты
   * Поставить 2 компа по 1 видеокарте в каждом
   * Вынести часть вычислений обратно на CPU
     * обычно на краях: в начале и/или в конце
 * compute bound branching
   * Использовать тернарки
   * Сделать 2 kernel'а, которые будут считать каждый свое, но в них не будет if'ов
   * Проводить вычисления над fake data, если не сработал condition if'а
   * Поиграться с размером кода внутри if'а
     * Сделать код внутри branch'а минимальным
     * Наоборот. Быть уверенным что branch не был сделан зря
       * Засунуть больше данных для вычисления
   * Переупорядочить данные
     * Если последовательно worksize потоков делают только один branch, то penalty не такой сильный
     * Уменьшить worksize
 * loc mem bound
   * Переходить на local
   * Уменьшать объем данных за счет wasted computation

## worksize (local)
Прим. для карт amd
 * 64 почти всегда плохо. -30% -- -50% производительности, но стоит один раз попробовать
 * **128** самый лучший вариант в среднем.
 * 256 обычно не дает преимущества, но стоит попрробывать

## Циклы
 * full unroll +5% perf, но в некоторых случаях при очень больших объемах кода просто не стартует
 * **1 for вполне ок**, amd opencl компилятор нормально оптимизирует такие циклы, но желательно в цикле иметь что-то тяжелое
   * доступ к памяти
   * много инструкций
 * Несколько вложенных циклов. Почти всегда плохо, но иногда без этого нельзя.
   * если есть возможность раскрыть внутренний цикл - неплохо

## if vs ternary vs mem access
 * тернарки всегда лучше if'ов на вычислениях
 * если доступ к памяти в одну и ту же ячейку со всех потоков. if плохо
 * если доступ к памяти с в разные ячейки if обычно полезен, но надо мерять

## local и barrier
 * **не уверен - не используй**
 * загрузить 1 раз в local, оградиться barrier(CLK_LOCAL_MEM_FENCE) и использовать много раз неплохой паттерн
   * [пример 1](https://github.com/brian112358/avermore-miner/blob/43e36dff5c21ad1ba98ef4b0efa07592780028d8/kernel/x16.cl#L348)
 * **не уверен - не используй** barrier(CLK_GLOBAL_MEM_FENCE)
   * Даже если уверен - не используй
   * Имеет смысл разбивать на 2 kernel'а

## atomics
 * **не уверен - не используй**
   * Заведи по переменной на каждый поток и пиши туда
   * Потом на CPU соберешь

## Способы работы с памятью
 * **straightforward pipeline strict**
   * host -> readonly buffer
   * readonly buffer -> kernel -> writeonly buffer
   * writeonly buffer -> host
 * **straightforward pipeline**
   * host -> readonly buffer
   * readonly buffer -> kernel -> read/write buffer
   * read/write buffer -> kernel -> writeonly buffer
   * writeonly buffer -> host
   * Прим. нежелательно читать и писать в один и тот же буффер одним и тем же kernel'ом
 * straightforward pipeline + dma trick
   * stage 1 host -> readonly buffer 1
   * stage 2
     * host -> readonly buffer 2
     * readonly buffer 1 -> kernel -> writeonly buffer 1
   * stage 3
     * host -> readonly buffer 1
     * readonly buffer 2 -> kernel -> writeonly buffer 2
     * writeonly buffer 1 -> host
 * По умолчанию не использовать 
   * CL_MEM_USE_HOST_PTR
   * CL_MEM_ALLOC_HOST_PTR
   * CL_MEM_COPY_HOST_PTR
   * Если у тебя данные не помещаются в оперативную память
     * Если на видеокарте <8Гб, купи видеокарту по-дороже
     * Разбей на куски и передавай через dma trick
     * А видеокарта таки дает ускорение по сравнению с например 64-ядерной системой типа H8QG6-F?
     * А не быстрее ли будет выполнить на AMD APU?
       * Смотреть на соотношение вычислительной мощности к необходимой пропускной способности к памяти
   * Когда таки использовать
     * HSA. Например AMD APU
       * В этом случае наоборот использовать всегда
     
## Идеальные заготовки
### 1 in - 1 out
 * low compute
   * vectorized read
   * vectorized write
   * ~10000 потоков (обычно x2-x3 от количества CU в видеокарте)
 * high compute
   * точно должны использоваться все CU
     * т.е. совет ~10000 потоков имеет первый приоритет
   * см. рецепты low compute
### N in - 1 out
 * обычно в таком случае несколько входов надодятся рядом
 * vectorized read
 * vertorized write
 * хорошие соотношения количества r/w
   * 8 r 4 w
   * 4+N r 4 w
 * если значение уже прочитали, то не надо его еще раз перечитывать с памяти
   * проще его прочитать из локальной переменной
   * Пример. Для подсчета res1 мы считали N=3
     * пускай в массиве glob хранятся значения в памяти
     * x0 = glob[0]
     * x1 = glob[1]
     * x2 = glob[2]
     * Для следующего элемента нам надо glob[1] glob[2] glob[3]
     * Медленно
       * x0 = glob[1]
       * x1 = glob[2]
       * x2 = glob[3]
     * Лучше
       * x0 = x1
       * x1 = x2
       * x2 = glob[3]
     * Еще лучше
       * изначально считывать еще больше x0 - x7
       * посчитать y1 - y6
       * x0 = x6
       * x1 = x7
       * для x2 - x7 читаем с glob
  * отдельный случай для случая когда вход `3*3` а выход 1 пиксель
    * в этом случае оптимальный layout tiling
    * Например мы читаем зону `8*8` пикселей а записываем в зону `5*5` пикселей
      * в этом случае КПД чтения 40%
      * если будем читать больше рискуем вылететь за пределы локальной памяти потока
      * если будем делать overlap через трюк x0=x1;x1=x2 то рискуем не насытить видеокарту достаточным количеством потоков
        * но имеет смысл если kernel memory bound

## Неидеальные заготовки
### buffer update (читаем и пишем в один буфер)
 * Если read rate ~100% от объема буфера и write rate >50%
   * Стоит попробовать завести 2 буфера
   * read 1 -> kernel -> write 2
   * write 1 <- kernel <- read 2
 * Если read rate < 50%
   * Возможно существует оракул который сможет угадывать какие зоны нужно читать а какие нет
   * Можно сразу запускать меньше потоков
   * Можно переписать kernel из стиля "всё принимаем, всё выдаем" на "draw call"
### Структура данных граф
 * В некоторых случаях сразу вешаться, или переводить на FPGA
 * Попробовать dense vs sparse варианты представления графа
 * Попробовать отсортировать вершины/ребра
   * Например по количеству ребер у вершины
   * Разбить на кластеры и над каждым проводить операции отдельным вызовом kernel'а или отдельным warp'ом
   * Попробовать сделать граф планарным или как-то использовать физические координаты вершины
 * Prefilter
   * Сначала отдельным вызовом kernel'а отфильтровать на каких вершинах/ребрах мы проводим вычисления и записать в массив
   * compact массива
     * опционально
     * можно попробовать сделать fusion с следующим
   * Собственно работа

## Прочие приемы оптимизации
 * kernel fusion
   * если доступы к памяти имеют +- одинаковый характер
   * если kernel 2 использует результаты kernel 1
   * имеет смысл объединить kernel 1 и kernel 2 и передавать данные через локальные переменные
 * compact data before use
   * сначала проходимся быстрым kernel'ом который перепакует данные которые лежат плохо
     * неупорядоченно
     * избыточно
       * например используется тип данных с 4 байтами вместо 1
       * ВАЖНО. rgba вместо rgb это плохой пример т.к. тут больше играет вопрос выровненности данных
     * неправильно сгрупировано
       * rgba-rgba-rgba-rgba вместо rrrr-gggg-bbbb
 * mul24 вместо `a*b`
 * mad24 вместо `a*b+c`
 * Запаковать данные побитово и делать побитовые операции над 32 битами вместо 32 операций над bool
      
## Антипаттерны и прочие мелочи
 * Полный reduce на видеокарте
   * Последние 16-256 элементов скорее всего по времени передачи на host не будут занимать много и на cpu досчитать все-равно не будет проблемы т.к. квант времени ОС большой.
 * Использование функций
   * Обычно хуже, чем inline ручками или через макросы
 * if или тернарка вместо min
   * См. микрокоды, там реально есть такая инструкция
 
